{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gatherting Initial Data\n",
    "CVALL_Directory = \"/Users/Andrew/Documents/FacialRecognitionRacialBias/Data/RawUnzippedFolders/CVALL\"\n",
    "\n",
    "# Prepping image transformations\n",
    "\n",
    "from torchvision import transforms\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "     transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Creating Total Dataset\n",
    "from torchvision import datasets\n",
    "all_dataset = datasets.ImageFolder(CVALL_Directory, transforms_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        # Batch normalization 1\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Convultional Layer 2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Batch Normalization 2\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Creating the Skip Connection\n",
    "        # By Default create an identity mapping\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # If stride != 1 then the spatial dimensions of the input need to be downsampled \n",
    "        # OR\n",
    "        # If input channels and output channels are not equal, then the shortcut connect must adjust the number of channels\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(input = x)\n",
    "        out = self.bn1(input = out)\n",
    "        out = F.relu(input = out, inplace= True)\n",
    "        out = self.conv2(input = out)\n",
    "        out = self.bn2(input = out)\n",
    "        shortcut = self.shortcut(input = x)\n",
    "        out += shortcut\n",
    "        out = F.relu(input = out, inplace= True)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, block_used, number_of_blocks, num_classes):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        #IMAGE: 224 x 224 X 3\n",
    "\n",
    "        # Initial Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels= 3, out_channels= 64, kernel_size= 7, stride= 2, padding = 1)\n",
    "        \n",
    "        # Batch Normalization & Max Pooling\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size= 3, stride = 2, padding = 1)\n",
    "\n",
    "        # Residual Block 1\n",
    "        self.ResidualBlock1 = self.make_layer(block_used, 64, number_of_blocks[0], stride = 1)\n",
    "        # Residual Block 2\n",
    "        self.ResidualBlock2 = self.make_layer(block_used, 128, number_of_blocks[1], stride = 2)\n",
    "        # Residual Block 3\n",
    "        self.ResidualBlock3 = self.make_layer(block_used, 256, number_of_blocks[2], stride = 2)\n",
    "        # Residual Block 4\n",
    "        # self.ResidualBlock4 = self.make_layer(block_used, 512, number_of_blocks[3], stride = 2)\n",
    "        \n",
    "        # Average Pool\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block_used, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block_used(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for block_to_create in range(1, blocks):\n",
    "            layers.append(block_used(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial Convolutional Layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Residual Blocks\n",
    "        x = self.ResidualBlock1(x)\n",
    "        x = self.ResidualBlock2(x)\n",
    "        x = self.ResidualBlock3(x)\n",
    "        # x = self.ResidualBlock4(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Helper Function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # move images to GPU \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propogate, compute predicted output based on images\n",
    "        outputs = model(images)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backwards propogate\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # calc num of correctly predicted samples\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Calculate average training loss and accuracy for epoch\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100.0 * correct / total\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Helper function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    # Set model to eval mode / tracking variables\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # disable gradient calc (since we are not training model we are validating / testing)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the batches\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "            # move images to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward propogate\n",
    "            outputs = model(images)\n",
    "\n",
    "            #calc loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # calculate running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calculate correct number of observations\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = 100.0 * correct / total\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Weights For Cross Entropy Due To Class Imbalances\n",
    "# Grab all labels\n",
    "targets = [sample[1] for sample in all_dataset.samples]\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = Counter(targets)\n",
    "total_samples = len(targets)\n",
    "class_weights = {class_idx: total_samples / count for class_idx, count in class_counts.items()}\n",
    "weights = [class_weights[i] for i in range(len(class_counts))]\n",
    "\n",
    "# Take calculated weights into a tensor\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Epoch: 1\n",
      "train loss: 1.3512050421457105 train_acc: 37.87540471139891\n",
      "val loss: 1.317456195089552 val_acc: 39.04889484259879\n",
      "Fold: 1 Epoch: 2\n",
      "train loss: 1.301352031715733 train_acc: 40.44881098582115\n",
      "val loss: 1.2831513166427613 val_acc: 42.01830765795937\n",
      "Fold: 1 Epoch: 3\n",
      "train loss: 1.2681714664926769 train_acc: 42.787763760187566\n",
      "val loss: 1.2472489734490713 val_acc: 44.764456351864254\n",
      "Fold: 1 Epoch: 4\n",
      "train loss: 1.2383607205906286 train_acc: 45.31092999888355\n",
      "val loss: 1.2446180595291985 val_acc: 43.62580933244028\n",
      "Fold: 1 Epoch: 5\n",
      "train loss: 1.2094815082868826 train_acc: 47.220051356480965\n",
      "val loss: 1.2076674898465474 val_acc: 44.630497878990845\n",
      "Fold: 1 Epoch: 6\n",
      "train loss: 1.1873892472647027 train_acc: 48.347661047225635\n",
      "val loss: 1.2165334754519992 val_acc: 46.483590087073004\n",
      "Fold: 1 Epoch: 7\n",
      "train loss: 1.1601015111862119 train_acc: 50.27352908339846\n",
      "val loss: 1.1910732997788323 val_acc: 47.57758428220585\n",
      "Fold: 1 Epoch: 8\n",
      "train loss: 1.1381874947826842 train_acc: 52.04309478620074\n",
      "val loss: 1.1085874802536435 val_acc: 52.77963831212324\n",
      "Fold: 1 Epoch: 9\n",
      "train loss: 1.1165817197318861 train_acc: 53.237691191247066\n",
      "val loss: 1.092474616236157 val_acc: 53.80665327081938\n",
      "Fold: 1 Epoch: 10\n",
      "train loss: 1.0997170284597986 train_acc: 54.047113989058836\n",
      "val loss: 1.1302992330657111 val_acc: 54.02991739227506\n",
      "Fold: 1 Epoch: 11\n",
      "train loss: 1.0764440140352276 train_acc: 55.66595958468237\n",
      "val loss: 1.078824782371521 val_acc: 53.78432685867381\n",
      "Fold: 1 Epoch: 12\n",
      "train loss: 1.0601558979175216 train_acc: 56.82706263257787\n",
      "val loss: 1.0092725687556796 val_acc: 58.96405447644563\n",
      "Fold: 1 Epoch: 13\n",
      "train loss: 1.0369759255132966 train_acc: 58.10539243050128\n",
      "val loss: 1.0612936112615796 val_acc: 56.19557937039518\n",
      "Fold: 1 Epoch: 14\n",
      "train loss: 1.0198018398125523 train_acc: 59.087864240259016\n",
      "val loss: 1.1601086798641416 val_acc: 52.935923197142216\n",
      "Fold: 1 Epoch: 15\n",
      "train loss: 1.0014812801873783 train_acc: 60.193145026236465\n",
      "val loss: 1.1642178611622915 val_acc: 51.46237999553472\n",
      "Fold: 1 Epoch: 16\n",
      "train loss: 0.9816182702364695 train_acc: 61.26493245506308\n",
      "val loss: 1.1065214296181998 val_acc: 53.80665327081938\n",
      "Fold: 1 Epoch: 17\n",
      "train loss: 0.9665599015429823 train_acc: 61.98503963380596\n",
      "val loss: 1.0733954346842236 val_acc: 57.26724715338245\n",
      "Fold: 1 Epoch: 18\n",
      "train loss: 0.94650126001629 train_acc: 62.94518253879647\n",
      "val loss: 0.9663513637251324 val_acc: 60.8841259209645\n",
      "Fold: 1 Epoch: 19\n",
      "train loss: 0.9319772934515164 train_acc: 63.62063190800491\n",
      "val loss: 0.8928650064600838 val_acc: 64.72426881000223\n",
      "Fold: 1 Epoch: 20\n",
      "train loss: 0.9141351678577306 train_acc: 64.50820587250196\n",
      "val loss: 1.0031330473721027 val_acc: 62.02277294038848\n",
      "Fold: 1 Epoch: 21\n",
      "train loss: 0.9021391757351442 train_acc: 65.00502400357263\n",
      "val loss: 0.9183731277783712 val_acc: 64.50100468854654\n",
      "Fold: 1 Epoch: 22\n",
      "train loss: 0.8806949214682938 train_acc: 65.94842022998772\n",
      "val loss: 0.920627682738834 val_acc: 63.78655949988837\n",
      "Fold: 1 Epoch: 23\n",
      "train loss: 0.8677691544995002 train_acc: 66.74667857541587\n",
      "val loss: 1.1972545133696662 val_acc: 52.868943960705515\n",
      "Fold: 1 Epoch: 24\n",
      "train loss: 0.8479516003125225 train_acc: 67.6063414089539\n",
      "val loss: 0.8653813676701652 val_acc: 66.44340254521099\n",
      "Fold: 1 Epoch: 25\n",
      "train loss: 0.8259305020892852 train_acc: 68.42134643295746\n",
      "val loss: 0.8554152564869987 val_acc: 67.18017414601474\n",
      "Fold: 1 Epoch: 26\n",
      "train loss: 0.8122319462903694 train_acc: 69.15820029027576\n",
      "val loss: 1.2746668348709742 val_acc: 51.953561062737215\n",
      "Fold: 1 Epoch: 27\n",
      "train loss: 0.7942580243671172 train_acc: 69.73875181422352\n",
      "val loss: 0.9100738702548875 val_acc: 66.1531591873186\n",
      "Fold: 1 Epoch: 28\n",
      "train loss: 0.7762795095656243 train_acc: 70.69331249302222\n",
      "val loss: 0.8474431597524219 val_acc: 67.87229292252735\n",
      "Fold: 1 Epoch: 29\n",
      "train loss: 0.7589752682404266 train_acc: 71.02266383833873\n",
      "val loss: 0.8222757667303086 val_acc: 67.67135521321724\n",
      "Fold: 1 Epoch: 30\n",
      "train loss: 0.7476835603667501 train_acc: 71.92140225521938\n",
      "val loss: 0.90519700050354 val_acc: 64.85822728287565\n",
      "Fold: 1 Epoch: 31\n",
      "train loss: 0.7369431813447256 train_acc: 72.12794462431617\n",
      "val loss: 0.9396391212940216 val_acc: 63.74190667559723\n",
      "Fold: 1 Epoch: 32\n",
      "train loss: 0.7165829260369198 train_acc: 72.59685162442783\n",
      "val loss: 0.9266802334123188 val_acc: 64.38937262781872\n",
      "Fold: 1 Epoch: 33\n",
      "train loss: 0.697488771507667 train_acc: 73.4174388746232\n",
      "val loss: 0.8228003889322281 val_acc: 68.22951551685644\n",
      "Fold: 1 Epoch: 34\n",
      "train loss: 0.689549452332069 train_acc: 73.87518142235123\n",
      "val loss: 1.2178305751747556 val_acc: 52.868943960705515\n",
      "Fold: 1 Epoch: 35\n",
      "train loss: 0.6787559123590464 train_acc: 74.39991068438093\n",
      "val loss: 0.8374206095933914 val_acc: 67.6267023889261\n",
      "Fold: 1 Epoch: 36\n",
      "train loss: 0.6581386447284879 train_acc: 75.15909344646646\n",
      "val loss: 0.8799439483218723 val_acc: 65.10381781647689\n",
      "Fold: 1 Epoch: 37\n",
      "train loss: 0.647208728498071 train_acc: 75.36005358937145\n",
      "val loss: 1.0914069006840388 val_acc: 58.65148470640768\n",
      "Fold: 1 Epoch: 38\n",
      "train loss: 0.6319065712454591 train_acc: 76.14156525622418\n",
      "val loss: 0.814509031176567 val_acc: 68.92163429336905\n",
      "Fold: 1 Epoch: 39\n",
      "train loss: 0.6058796012467993 train_acc: 77.25242826839343\n",
      "val loss: 0.8249019470479754 val_acc: 68.31882116543872\n",
      "Fold: 1 Epoch: 40\n",
      "train loss: 0.5910277369128628 train_acc: 77.83297979234119\n",
      "val loss: 0.7592248378528489 val_acc: 71.82406787229293\n",
      "Fold: 1 Epoch: 41\n",
      "train loss: 0.5860330672483258 train_acc: 78.00044657809535\n",
      "val loss: 0.7573074334197574 val_acc: 71.35521321723598\n",
      "Fold: 1 Epoch: 42\n",
      "train loss: 0.5722266134279352 train_acc: 78.3744557329463\n",
      "val loss: 0.769903973241647 val_acc: 70.88635856217905\n",
      "Fold: 1 Epoch: 43\n",
      "train loss: 0.550742991538433 train_acc: 79.08339845930557\n",
      "val loss: 0.9383615949087672 val_acc: 65.6619781201161\n",
      "Fold: 1 Epoch: 44\n",
      "train loss: 0.5329821211879964 train_acc: 80.11052807859774\n",
      "val loss: 0.8268644023272727 val_acc: 69.72538513060951\n",
      "Fold: 1 Epoch: 45\n",
      "train loss: 0.5196820138058622 train_acc: 80.02121245952885\n",
      "val loss: 0.928284210463365 val_acc: 65.46104041080598\n",
      "Fold: 1 Epoch: 46\n",
      "train loss: 0.5054091538061338 train_acc: 80.32265267388635\n",
      "val loss: 1.1218047012885413 val_acc: 58.78544317928109\n",
      "Fold: 1 Epoch: 47\n",
      "train loss: 0.4884873636087667 train_acc: 81.45584459082282\n",
      "val loss: 0.7234827823109097 val_acc: 73.05202054029917\n",
      "Fold: 1 Epoch: 48\n",
      "train loss: 0.46871726795989493 train_acc: 82.35458300770347\n",
      "val loss: 0.989323299874862 val_acc: 64.47867827640098\n",
      "Fold: 1 Epoch: 49\n",
      "train loss: 0.4571079600571258 train_acc: 82.71742771017081\n",
      "val loss: 0.8497716137104564 val_acc: 69.63607948202724\n",
      "Fold: 1 Epoch: 50\n",
      "train loss: 0.43533719644407043 train_acc: 83.52126828179078\n",
      "val loss: 0.9346926013628641 val_acc: 65.37173476222371\n",
      "Fold: 2 Epoch: 1\n",
      "train loss: 1.3480764517877095 train_acc: 35.18477168694876\n",
      "val loss: 1.3205752266777886 val_acc: 39.51774949765573\n",
      "Fold: 2 Epoch: 2\n",
      "train loss: 1.298692364546582 train_acc: 40.06363737858658\n",
      "val loss: 1.2922105285856458 val_acc: 41.192230408573344\n",
      "Fold: 2 Epoch: 3\n",
      "train loss: 1.2677188181943548 train_acc: 42.57563916489896\n",
      "val loss: 1.2577048910988702 val_acc: 43.290913150256756\n",
      "Fold: 2 Epoch: 4\n",
      "train loss: 1.2424004947906748 train_acc: 44.434520486770126\n",
      "val loss: 1.2498393621709611 val_acc: 44.18396963607948\n",
      "Fold: 2 Epoch: 5\n",
      "train loss: 1.2186598591817788 train_acc: 46.23757954672323\n",
      "val loss: 1.2076040579213037 val_acc: 46.88546550569323\n",
      "Fold: 2 Epoch: 6\n",
      "train loss: 1.1908935119182618 train_acc: 48.44814111867813\n",
      "val loss: 1.2389867179923588 val_acc: 45.25563741906676\n",
      "Fold: 2 Epoch: 7\n",
      "train loss: 1.1651192923442235 train_acc: 50.71452495255108\n",
      "val loss: 1.2214708692497678 val_acc: 45.72449207412369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(val_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Train model with current train fold\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     train_loss, train_acc, \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Validate Model with current validation fold\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move images to GPU \u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\PIL\\Image.py:934\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    932\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matrix):\n\u001b[1;32m--> 934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;66;03m# matrix conversion\u001b[39;00m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\Documents\\FacialRecognitionRacialBias\\.venv\\Lib\\site-packages\\PIL\\Image.py:1192\u001b[0m, in \u001b[0;36mImage.copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;124;03mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03minto an image, but still retain the original.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize All Dataset\n",
    "\n",
    "# Stratified K Fold \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "num_epochs = 50\n",
    "targets = [sample[1] for sample in all_dataset.samples]\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "    # Initialize Model Criterion Optimizer\n",
    "    device = \"cuda\"\n",
    "    model = ResidualCNN(ResidualBlock, [1,1,1,1], 4).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "    # grab subset for training based on train indices returned by skf.split()\n",
    "    train_subset = Subset(all_dataset, train_idx)\n",
    "\n",
    "    # grab subset for validation based on train indices returned by skf.split()\n",
    "    val_subset = Subset(all_dataset, val_idx)\n",
    "\n",
    "    # Now properly Load Data into ingestible form for ResNet\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=50, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=50, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train model with current train fold\n",
    "        train_loss, train_acc, = train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate Model with current validation fold\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Fold: {fold + 1} Epoch: {epoch + 1}\")\n",
    "        print(f\"train loss: {train_loss} train_acc: {train_acc}\")\n",
    "        print(f\"val loss: {val_loss} val_acc: {val_acc}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
